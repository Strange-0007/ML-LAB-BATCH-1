{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4474baf9",
   "metadata": {},
   "source": [
    "\n",
    "# Opinion‑BERT Microservice (Prototype) — FastAPI + OpenAPI on Colab\n",
    "This notebook spins up a **FastAPI** microservice (with OpenAPI docs) that mimics the API surface for an Opinion‑BERT style model (multi‑task: sentiment + mental‑health status).  \n",
    "It uses **DistilBERT** for sentiment + **TextBlob/VADER** for opinion features as a **baseline**. You can later swap in your Hybrid BERT + CNN + BiGRU model.\n",
    "\n",
    "**Endpoints:**\n",
    "- `GET /health` — liveness check  \n",
    "- `GET /ready` — readiness + model load time  \n",
    "- `POST /sentiment/analyze` — main inference (sentiment + status + opinion features)\n",
    "\n",
    "> ⚠️ This is **not medical advice** and **not a diagnostic tool**. It’s a research prototype for software integration only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Step 1: Install dependencies\n",
    "# (FastAPI, Uvicorn, Transformers, PyTorch, TextBlob, VADER, Cloudflared, etc.)\n",
    "!pip -q install fastapi \"uvicorn[standard]\" transformers torch textblob vaderSentiment nest-asyncio starlette >/dev/null\n",
    "\n",
    "# Cloudflared (for public URL without auth)\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "!sudo dpkg -i cloudflared-linux-amd64.deb >/dev/null 2>&1 || true\n",
    "\n",
    "# Optional (sometimes helpful): download TextBlob corpora\n",
    "# !python -m textblob.download_corpora\n",
    "print(\"✅ Dependencies installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Step 2: Create the FastAPI app (app.py)\n",
    "%%writefile app.py\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict\n",
    "import time\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "APP_TITLE = \"Opinion‑BERT Microservice (Prototype)\"\n",
    "APP_VERSION = \"0.1.0\"\n",
    "APP_DESC = (\n",
    "    \"Baseline service exposing an Opinion‑BERT‑style API surface with OpenAPI. \"\n",
    "    \"Implements sentiment + status (heuristic) + opinion features. Replace the model later.\"\n",
    ")\n",
    "\n",
    "app = FastAPI(title=APP_TITLE, version=APP_VERSION, description=APP_DESC)\n",
    "\n",
    "# CORS (open for demo; restrict for production)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class AnalyzeRequest(BaseModel):\n",
    "    text: str = Field(..., min_length=1, max_length=4000)\n",
    "    context: Optional[List[str]] = Field(default=None, description=\"Optional previous messages for context.\")\n",
    "    return_explanations: bool = Field(default=False, description=\"Reserved for saliency/attention outputs later.\")\n",
    "\n",
    "class TaskScore(BaseModel):\n",
    "    label: str\n",
    "    score: float\n",
    "\n",
    "class OpinionFeatures(BaseModel):\n",
    "    polarity: float\n",
    "    subjectivity: float\n",
    "    vader: Dict[str, float]\n",
    "\n",
    "class AnalyzeResponse(BaseModel):\n",
    "    sentiment: TaskScore\n",
    "    status: TaskScore\n",
    "    opinion: OpinionFeatures\n",
    "    timings_ms: Dict[str, float]\n",
    "    model: Dict[str, str]\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def load_model():\n",
    "    # Load baseline components\n",
    "    start = time.time()\n",
    "    app.state.sentiment = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    )\n",
    "    app.state.vader = SentimentIntensityAnalyzer()\n",
    "    app.state.load_time_ms = int((time.time() - start) * 1000)\n",
    "\n",
    "@app.get(\"/health\", tags=[\"ops\"])\n",
    "def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@app.get(\"/ready\", tags=[\"ops\"])\n",
    "def ready():\n",
    "    return {\"ready\": True, \"load_time_ms\": getattr(app.state, \"load_time_ms\", None)}\n",
    "\n",
    "@app.post(\"/sentiment/analyze\", response_model=AnalyzeResponse, tags=[\"inference\"])\n",
    "def analyze(req: AnalyzeRequest):\n",
    "    t0 = time.time()\n",
    "    tb = TextBlob(req.text)\n",
    "    pol = float(tb.sentiment.polarity)\n",
    "    sub = float(tb.sentiment.subjectivity)\n",
    "    t_tb = (time.time() - t0) * 1000\n",
    "\n",
    "    t1 = time.time()\n",
    "    vs = app.state.vader.polarity_scores(req.text)\n",
    "    t_vader = (time.time() - t1) * 1000\n",
    "\n",
    "    t2 = time.time()\n",
    "    out = app.state.sentiment(req.text)[0]\n",
    "    # Normalize labels to lowercase \"positive\"/\"negative\"\n",
    "    label = \"positive\" if \"POS\" in out[\"label\"].upper() else \"negative\"\n",
    "    conf = float(out[\"score\"])\n",
    "    t_hf = (time.time() - t2) * 1000\n",
    "\n",
    "    # Heuristic \"status\" head to mimic multi-task output (replace with your trained head later)\n",
    "    if label == \"negative\" or pol < -0.2 or vs[\"compound\"] < -0.35:\n",
    "        status_label = \"at_risk\"\n",
    "        # naive score: higher when negativity cues increase\n",
    "        status_score = min(1.0, max(0.0, 0.5 + (abs(min(0, pol)) + abs(min(0, vs['compound']))) / 1.5))\n",
    "    else:\n",
    "        status_label = \"well\"\n",
    "        status_score = conf\n",
    "\n",
    "    return AnalyzeResponse(\n",
    "        sentiment=TaskScore(label=label, score=conf),\n",
    "        status=TaskScore(label=status_label, score=float(status_score)),\n",
    "        opinion=OpinionFeatures(polarity=pol, subjectivity=sub, vader=vs),\n",
    "        timings_ms={\"textblob\": t_tb, \"vader\": t_vader, \"hf\": t_hf},\n",
    "        model={\n",
    "            \"encoder\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "            \"service_version\": APP_VERSION,\n",
    "            \"note\": \"Heuristic multi-task head. Swap with Hybrid BERT+CNN+BiGRU later.\"\n",
    "        },\n",
    "    )\n",
    "print(\"✅ Wrote app.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Step 3: Launch the FastAPI app (Uvicorn) in the background\n",
    "import nest_asyncio, uvicorn, threading, time\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def _run():\n",
    "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "thread = threading.Thread(target=_run, daemon=True)\n",
    "thread.start()\n",
    "time.sleep(2)\n",
    "print(\"✅ Uvicorn started on http://127.0.0.1:8000  (next cell will create a public URL)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312406f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Step 4: Expose the API publicly (no account needed)\n",
    "import re, time, os, subprocess, pathlib\n",
    "\n",
    "# Start Cloudflared tunnel in background and log output to file\n",
    "log_path = \"cf.log\"\n",
    "if os.path.exists(log_path):\n",
    "    os.remove(log_path)\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "    [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8000\", \"--no-autoupdate\"],\n",
    "    stdout=open(log_path, \"w\"),\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Wait for the public URL to appear\n",
    "public_url = None\n",
    "for _ in range(30):  # ~30 * 0.5s = 15s max wait\n",
    "    time.sleep(0.5)\n",
    "    if os.path.exists(log_path):\n",
    "        txt = open(log_path).read()\n",
    "        m = re.search(r\"https://[a-zA-Z0-9-]+\\.trycloudflare\\.com\", txt)\n",
    "        if m:\n",
    "            public_url = m.group(0)\n",
    "            break\n",
    "\n",
    "if public_url:\n",
    "    print(\"🌍 Public URL:\", public_url)\n",
    "    print(\"📚 OpenAPI docs:\", public_url + \"/docs\")\n",
    "else:\n",
    "    print(\"⚠️ Could not detect public URL. Scroll up to the cell output; Cloudflared prints it early.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0aa94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Step 5: Smoke test (change `PUBLIC` if needed)\n",
    "import json, requests\n",
    "\n",
    "try:\n",
    "    PUBLIC = public_url  # from previous cell\n",
    "except NameError:\n",
    "    PUBLIC = \"http://127.0.0.1:8000\"\n",
    "\n",
    "print(\"Testing against:\", PUBLIC)\n",
    "\n",
    "health = requests.get(PUBLIC + \"/health\", timeout=10).json()\n",
    "ready = requests.get(PUBLIC + \"/ready\", timeout=10).json()\n",
    "demo = requests.post(PUBLIC + \"/sentiment/analyze\",\n",
    "                     json={\"text\": \"I feel hopeless and exhausted. Nothing seems to help.\"},\n",
    "                     timeout=15).json()\n",
    "\n",
    "print(\"GET /health ->\", health)\n",
    "print(\"GET /ready  ->\", ready)\n",
    "print(\"POST /sentiment/analyze ->\")\n",
    "print(json.dumps(demo, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f6f1f",
   "metadata": {},
   "source": [
    "\n",
    "## 🔄 Swap in your Hybrid BERT + CNN + BiGRU (later)\n",
    "- Replace the `pipeline(\"sentiment-analysis\", ...)` with your **BERT encoder**.\n",
    "- Concatenate **opinion embeddings** computed from the text (or integrate as cross-features).\n",
    "- Add **CNN + BiGRU** layers and **two heads**: `sentiment` and `status`.\n",
    "- Keep the same **Pydantic request/response** models so the API remains stable.\n",
    "- Use **MLflow** to version models and **Redis** to cache hot predictions.\n",
    "- Expose model metadata under `model` in the response.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}